timezone: Asia/Tokyo

_export:
  wf:
    name: Digdag Workflow for Site Monthly Report to Get Ranking

  bq:
    dataset_name: temp
    table_name1: tat_site_ranking_uu_web_temp
    destination1: gs://stk-bigquery-export/tat_site_ranking_uu_web_temp*.csv.gz
    query1: queries/BQ_RANKING/RANKING_UU_WEB.sql
    table_name2: tat_site_ranking_uu_app_temp
    destination2: gs://stk-bigquery-export/tat_site_ranking_uu_app_temp*.csv.gz
    query2: queries/BQ_RANKING/RANKING_UU_APP.sql

  embulk:
    yml_path1: embulk/guessed_ranking_uu_web.yml
    file_path1: /tmp/embulk/tat_site_ranking_uu_web
    file_prefix1: tat_site_ranking_uu_web_
    out_file1: loadfile_tat_site_ranking_uu_web.csv
    yml_path2: embulk/guessed_ranking_uu_app.yml
    file_path2: /tmp/embulk/tat_site_ranking_uu_app
    file_prefix2: tat_site_ranking_uu_app_
    out_file2: loadfile_tat_site_ranking_uu_app.csv
    key_file_path: /var/lib/digdag/.credential/gcpcredential.json

  pure:
    jar: /var/lib/digdag/puredata/nzExecuteSql-1.0.1.jar
    class: jp.stk.cfm.ExecuteSqls
    properties: /var/lib/digdag/puredata/dbconnection.properties
    query_dir: queries/PD_RANKING

  my_param:
    - DIGDAGSERVER_HOST
    - DIGDAGSERVER_ENV

  slack:
    webhook: https://hooks.slack.com/services/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
    channel: '#zozo-anablock'
    username: digdag
    icon_emoji: ghost
    template_path: notification
    good: good-template.yml
    danger: danger-template.yml

  plugin:
    repositories:
      - https://jitpack.io
    dependencies:
      - com.github.szyn:digdag-slack:0.1.2
  # Set Reqired params
  webhook_url: ${slack.webhook}
  # Set Option params
  workflow_name: ${wf.name}

+prepare_environments:
  py>: tasks.PrepareEnviroments.set_parameters

_error:
  slack>: ${slack.template_path}/${slack.danger}

+start:
  echo>: start ${moment(session_time).utc().format('YYYY-MM-DD HH:mm:ss Z')}

+get_first_bqdata:
  +create_table:
    +pre_delete_gcs_file:
      sh>: gsutil rm -rf ${bq.destination1} || echo "no target file at the bucket"

    +pre_delete:
      bq_ddl>:
      delete_tables:
        - ${bq.dataset_name}.${bq.table_name1}

    +create_select:
      bq>: ${bq.query1}
      use_legacy_sql: true
      allow_large_results: True
      dataset: ${bq.dataset_name}
      destination_table: ${bq.table_name1}

  +extract_gcs:
    +extract_file:
      bq_extract>: ${bq.dataset_name}.${bq.table_name1}
      destination: ${bq.destination1}
      compression: GZIP
      destination_format: CSV

    +post_delete:
      bq_ddl>:
      delete_tables:
        - ${bq.dataset_name}.${bq.table_name1}

  +download_file:
    +prepare_dir:
      sh>: mkdir -p ${embulk.file_path1}

    +download_local:
      embulk>: ${embulk.yml_path1}

    +merge_file:
      sh>: cat ${embulk.file_path1}/${embulk.file_prefix1}* > ${embulk.file_path1}/${embulk.out_file1} && rm -r ${embulk.file_path1}/${embulk.file_prefix1}*

    +post_delete_gcs_file:
      sh>: gsutil rm -rf ${bq.destination1} || echo "no target file at the bucket"

+get_second_bqdata:
  +create_table:
    +pre_delete_gcs_file:
      sh>: gsutil rm -rf ${bq.destination2} || echo "no target file at the bucket"

    +pre_delete:
      bq_ddl>:
      delete_tables:
        - ${bq.dataset_name}.${bq.table_name2}

    +create_select:
      bq>: ${bq.query2}
      use_legacy_sql: true
      allow_large_results: True
      dataset: ${bq.dataset_name}
      destination_table: ${bq.table_name2}

  +extract_gcs:
    +extract_file:
      bq_extract>: ${bq.dataset_name}.${bq.table_name2}
      destination: ${bq.destination2}
      compression: GZIP
      destination_format: CSV

    +post_delete:
      bq_ddl>:
      delete_tables:
        - ${bq.dataset_name}.${bq.table_name2}

  +download_file:
    +prepare_dir:
      sh>: mkdir -p ${embulk.file_path2}

    +download_local:
      embulk>: ${embulk.yml_path2}

    +merge_file:
      sh>: cat ${embulk.file_path2}/${embulk.file_prefix2}* > ${embulk.file_path2}/${embulk.out_file2} && rm -r ${embulk.file_path2}/${embulk.file_prefix2}*

    +post_delete_gcs_file:
      sh>: gsutil rm -rf ${bq.destination2} || echo "no target file at the bucket"

+insert_puredata:
  +load_sql:
    sh>: java -cp ${pure.jar} ${pure.class} ${pure.query_dir} ${pure.properties}

  +remove_file:
    sh>: rm -rf ${embulk.file_path1} && rm -rf ${embulk.file_path2}

+teardown:
  echo>: finish ${moment(session_time).utc().format('YYYY-MM-DD HH:mm:ss Z')}
